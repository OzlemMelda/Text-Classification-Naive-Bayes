{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(D):\n",
    "    \n",
    "    \"\"\"\n",
    "    The vocabulary is a set of tokens which appear more than once in the entire\n",
    "    document collection plus the \"<unk>\" token.\n",
    "    \n",
    "    Args:\n",
    "        D (list): A list of documents, where each document is represented as a list of tokens.\n",
    "        \n",
    "    Returns:\n",
    "        vocabulary (set): A set of tokens.\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    # get words to vocabulary if repeated more than once\n",
    "    min_threshold = 1 \n",
    "    D = [j for i in D for j in i]\n",
    "    D_count = Counter(D)\n",
    "    vocab = {k for k,v in D_count.items() if v > min_threshold}\n",
    "    vocab.add('unk')\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBoWFeaturizer(object):\n",
    "    \n",
    "    def convert_document_to_feature_dictionary(self, doc, vocab):\n",
    "        \n",
    "        \"\"\"\n",
    "        This function returns a dictionary which maps from the name of the\n",
    "        feature to the value of that feature.\n",
    "        \n",
    "        Args:\n",
    "        doc (list): A list of tokens.\n",
    "        vocab (set): A set of tokens.\n",
    "        \n",
    "        Returns:\n",
    "        dictionary (dict): A feature representation.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        # assign value of 1 to words present in vocab\n",
    "        feature_dictionary = {k:1 for k in doc if k in vocab} \n",
    "        \n",
    "        unk_count = len(set(doc).difference(vocab)) # handle unk\n",
    "        if unk_count > 0:\n",
    "            feature_dictionary['unk'] = 1\n",
    "        \n",
    "        return feature_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBoWFeaturizer(object):\n",
    "    \n",
    "    def convert_document_to_feature_dictionary(self, doc, vocab):\n",
    "        \n",
    "        \"\"\"\n",
    "        This function computes the count bag-of-words feature representation.\n",
    "        and returns a dictionary which maps from the name of the\n",
    "        feature to the value of that feature.\n",
    "        \n",
    "        Args:\n",
    "        doc (list): A list of tokens.\n",
    "        vocab (set): A set of tokens.\n",
    "        \n",
    "        Returns:\n",
    "        dictionary (dict): A feature representation.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        counter = Counter(doc)\n",
    "        \n",
    "        # assign freq to words present in vocab\n",
    "        feature_dictionary = {k:l for k,l in counter.items() if k in vocab} \n",
    "        \n",
    "        unk_count = len(set(doc).difference(vocab)) # handle unk\n",
    "        if unk_count > 0:\n",
    "            feature_dictionary['unk'] = unk_count\n",
    "        \n",
    "        return feature_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(D, vocab):\n",
    "    \n",
    "    \"\"\"\n",
    "    Where each document is represented as a list of tokens, \n",
    "    this function returns the IDF scores for every token in the vocab. \n",
    "    The IDFs is represented as a dictionary that \n",
    "    maps from the token to the IDF value. \n",
    "    If a token is not present in the vocab, it is mapped to \"<unk>\".\n",
    "    \n",
    "    Args:\n",
    "    D (list): A list of documents.\n",
    "    vocab (set): A set of tokens.\n",
    "\n",
    "    Returns:\n",
    "    idf (dict): IDF value.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    D_len = len(D)\n",
    "    \n",
    "    idf = {}\n",
    "    \n",
    "    for v in vocab:\n",
    "        idf[v] = 0\n",
    "    \n",
    "    idf_unk_temp = 0\n",
    "    for d in D:\n",
    "        idf['unk'] += idf_unk_temp  # handle unk\n",
    "        idf_unk_temp = 0\n",
    "        for v in set(d): # unique set of tokens in a single doc d\n",
    "            if v in vocab:\n",
    "                idf[v] += 1\n",
    "            if v not in vocab:\n",
    "                idf_unk_temp = 1\n",
    "                \n",
    "    for v in vocab:\n",
    "        idf[v] = np.log(D_len / idf[v])\n",
    "    \n",
    "    return idf\n",
    "    \n",
    "class TFIDFFeaturizer(object):\n",
    "    \n",
    "    def __init__(self, idf):\n",
    "        \n",
    "        \"\"\"The idf scores computed via `compute_idf`.\"\"\"\n",
    "        \n",
    "        self.idf = idf\n",
    "    \n",
    "    def convert_document_to_feature_dictionary(self, doc, vocab):\n",
    "        \n",
    "        \"\"\"\n",
    "        This function computes the TF-IDF feature representation \n",
    "        and return a dictionary which maps from the name of the\n",
    "        feature to the value of that feature.\n",
    "        \n",
    "        Args:\n",
    "        doc (list): A list of tokens.\n",
    "        vocab (set): A set of tokens.\n",
    "\n",
    "        Returns:\n",
    "        tf_idf (dict): TF-IDF feature.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        counter = Counter(doc)\n",
    "        \n",
    "        # assign freq to words present in vocab\n",
    "        tf = {k:l for k,l in counter.items() if k in vocab} \n",
    "        \n",
    "        unk_count = len(set(doc).difference(vocab))  # handle unk\n",
    "        if unk_count > 0:\n",
    "            tf['unk'] = unk_count\n",
    "        \n",
    "        tf_idf = {}\n",
    "        for v in tf.keys():\n",
    "            tf_idf[v] = tf[v] * idf[v]\n",
    "        \n",
    "        return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "def load_dataset(file_path):\n",
    "    D = []\n",
    "    y = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            instance = json.loads(line)\n",
    "            D.append(instance['document'])\n",
    "            y.append(instance['label'])\n",
    "    return D, y\n",
    "\n",
    "def convert_to_features(D, featurizer, vocab):\n",
    "    X = []\n",
    "    for doc in D:\n",
    "        X.append(featurizer.convert_document_to_feature_dictionary(doc, vocab))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(X, y, k, vocab):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Computes the statistics for the Naive Bayes classifier.\n",
    "    \n",
    "    Args:\n",
    "    X (list): A list of feature representations, where each representation\n",
    "    is a dictionary that maps from the feature name to the value.\n",
    "    y (list): A list of integers that represent the labels.\n",
    "    k (float): A float which is the smoothing parameters.\n",
    "    vocab (set): A set of vocabulary tokens.\n",
    "    \n",
    "    Returns:\n",
    "        p_y: A dictionary from the label to the corresponding p(y) score\n",
    "        p_v_y: A nested dictionary where the outer dictionary's key is\n",
    "            the label and the innner dictionary maps from a feature\n",
    "            to the probability p(v|y). For example, `p_v_y[1][\"hello\"]`\n",
    "            should be p(v=\"hello\"|y=1).\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate prior probability\n",
    "    y_unique = list(set(y))\n",
    "    p_y = {}\n",
    "    for i in y_unique:\n",
    "        p_y[i] = sum(1 for x in y if x == i)/len(X)\n",
    "    \n",
    "    # Calculate conditional probability\n",
    "    p_v_y = {}\n",
    "    V_d = len(vocab)\n",
    "    for j in y_unique:\n",
    "        p_v_y[j] = {} # create nested dict\n",
    "        for v in vocab:\n",
    "            p_v_y[j][v] = 0\n",
    "\n",
    "        # choose a class\n",
    "        y_c = [c == j for c in y]  \n",
    "        # filter X based on chosen class\n",
    "        x_c = [_x for _x, _y in zip(X, y_c) if _y == True]  \n",
    "\n",
    "        c = Counter()\n",
    "        denom = 0\n",
    "        for d in x_c:\n",
    "            c.update(d) # calculate numerator\n",
    "            denom += sum(d.values()) # calculate denominator\n",
    "        num = dict(c)\n",
    "        \n",
    "        # numerator value update\n",
    "        p_v_y[j] = {i: p_v_y[j].get(i, 0) + num.get(i, 0) for i in set(p_v_y[j]).union(num)} \n",
    "\n",
    "        # calculate cond prob by numerator / denominator\n",
    "        for v in vocab:\n",
    "            p_v_y[j][v] = (p_v_y[j][v] + k)/(denom + (k * V_d)) \n",
    "    \n",
    "    return p_y, p_v_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_naive_bayes(D, p_y, p_v_y):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Runs the prediction rule for Naive Bayes.\n",
    "    \n",
    "    Note that any token which is not in p_v_y is mapped to\n",
    "    \"<unk>\". Further, the input dictionaries are probabilities. They\n",
    "    are converted to log-probabilities while computing\n",
    "    the Naive Bayes prediction rule to prevent underflow errors.\n",
    "    \n",
    "    Args:\n",
    "    D (list): A list of documents, where each document is a list of tokens.\n",
    "    p_y (list): Output from `train_naive_bayes`.\n",
    "    p_v_y (float): Output from `train_naive_bayes`.\n",
    "    \n",
    "    Returns:\n",
    "        predictions: A list of integer labels, one for each document,\n",
    "            that is the predicted label for each instance.\n",
    "        confidences: A list of floats, one for each document, that is\n",
    "            p(y|d) for the corresponding label that is returned.\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    for d in D:\n",
    "        max_obj = {}\n",
    "        for y in p_y.keys():\n",
    "            max_obj[y] = p_y[y]\n",
    "            for v in d:\n",
    "                max_obj[y] += np.log(p_v_y[y][v] + 0.00000000000001) # handle 0 log -> -inf\n",
    "        predictions.append(max(max_obj, key=max_obj.get))\n",
    "        confidences.append(max(max_obj.values()))\n",
    "    \n",
    "    return predictions, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables that are named D_* are lists of documents where each\n",
    "# document is a list of tokens. y_* is a list of integer class labels.\n",
    "# X_* is a list of the feature dictionaries for each document.\n",
    "D_train, y_train = load_dataset('data/imdb/train.jsonl')\n",
    "D_valid, y_valid = load_dataset('data/imdb/valid.jsonl')\n",
    "D_test, y_test = load_dataset('data/imdb/test.jsonl')\n",
    "\n",
    "vocab = get_vocabulary(D_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBoWFeaturizer...\n",
      "k:  0.001\n",
      "validation accuracy for k 0.001 is  0.8584\n",
      "k:  0.01\n",
      "validation accuracy for k 0.01 is  0.8648\n",
      "k:  0.1\n",
      "validation accuracy for k 0.1 is  0.8672\n",
      "k:  1.0\n",
      "validation accuracy for k 1.0 is  0.8612\n",
      "k:  10.0\n",
      "validation accuracy for k 10.0 is  0.8624\n",
      "Chosen k for BBoWFeaturizer is  0.1\n",
      "CBoWFeaturizer...\n",
      "k:  0.001\n",
      "validation accuracy for k 0.001 is  0.8556\n",
      "k:  0.01\n",
      "validation accuracy for k 0.01 is  0.8616\n",
      "k:  0.1\n",
      "validation accuracy for k 0.1 is  0.8632\n",
      "k:  1.0\n",
      "validation accuracy for k 1.0 is  0.8616\n",
      "k:  10.0\n",
      "validation accuracy for k 10.0 is  0.8604\n",
      "Chosen k for CBoWFeaturizer is  0.1\n",
      "TFIDFFeaturizer...\n",
      "k:  0.001\n",
      "validation accuracy for k 0.001 is  0.8308\n",
      "k:  0.01\n",
      "validation accuracy for k 0.01 is  0.8348\n",
      "k:  0.1\n",
      "validation accuracy for k 0.1 is  0.8368\n",
      "k:  1.0\n",
      "validation accuracy for k 1.0 is  0.8352\n",
      "k:  10.0\n",
      "validation accuracy for k 10.0 is  0.8384\n",
      "Chosen k for TFIDFFeaturizer is  10.0\n"
     ]
    }
   ],
   "source": [
    "# Training and Prediction\n",
    "\n",
    "# UNK prep\n",
    "for i in range(0,len(D_valid)):\n",
    "    for j in range(0,len(D_valid[i])):\n",
    "        if D_valid[i][j] not in vocab:\n",
    "            D_valid[i][j] = 'unk'\n",
    "\n",
    "# Hyperparameter selection\n",
    "k = [0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "print('BBoWFeaturizer...')\n",
    "best_acc = 0\n",
    "for k_c in k:\n",
    "    print('k: ', k_c)\n",
    "    featurizer = BBoWFeaturizer()\n",
    "    X_train = convert_to_features(D_train, featurizer, vocab)\n",
    "    p_y, p_v_y = train_naive_bayes(X_train, y_train, k_c, vocab)\n",
    "    preds, conf = predict_naive_bayes(D_valid, p_y, p_v_y)\n",
    "    acc = accuracy_score(preds, y_valid)\n",
    "    print('validation accuracy for k ' + str(k_c) + ' is ', acc)\n",
    "    if acc > best_acc:\n",
    "        BBoW_k = k_c\n",
    "        best_acc = acc\n",
    "print('Chosen k for BBoWFeaturizer is ', BBoW_k)   \n",
    "\n",
    "print('CBoWFeaturizer...')\n",
    "best_acc = 0\n",
    "for k_c in k:\n",
    "    print('k: ', k_c)\n",
    "    featurizer = CBoWFeaturizer()\n",
    "    X_train = convert_to_features(D_train, featurizer, vocab)\n",
    "    p_y, p_v_y = train_naive_bayes(X_train, y_train, k_c, vocab)\n",
    "    preds, conf = predict_naive_bayes(D_valid, p_y, p_v_y)\n",
    "    acc = accuracy_score(preds, y_valid)\n",
    "    print('validation accuracy for k ' + str(k_c) + ' is ', acc)\n",
    "    if acc > best_acc:\n",
    "        CBoW_k = k_c\n",
    "        best_acc = acc\n",
    "print('Chosen k for CBoWFeaturizer is ', CBoW_k) \n",
    "\n",
    "print('TFIDFFeaturizer...')\n",
    "best_acc = 0\n",
    "idf = compute_idf(D_train, vocab)\n",
    "for k_c in k:\n",
    "    print('k: ', k_c)\n",
    "    featurizer = TFIDFFeaturizer(idf)\n",
    "    X_train = convert_to_features(D_train, featurizer, vocab)\n",
    "    p_y, p_v_y = train_naive_bayes(X_train, y_train, k_c, vocab)\n",
    "    preds, conf = predict_naive_bayes(D_valid, p_y, p_v_y)\n",
    "    acc = accuracy_score(preds, y_valid)\n",
    "    print('validation accuracy for k ' + str(k_c) + ' is ', acc)\n",
    "    if acc > best_acc:\n",
    "        TFIDF_k = k_c\n",
    "        best_acc = acc\n",
    "print('Chosen k for TFIDFFeaturizer is ', TFIDF_k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with chosen parameter k on train & validation data \n",
    "# and test the model on test data\n",
    "\n",
    "# UNK prep\n",
    "for i in range(0,len(D_test)):\n",
    "    for j in range(0,len(D_test[i])):\n",
    "        if D_test[i][j] not in vocab:\n",
    "            D_test[i][j] = 'unk'\n",
    "    \n",
    "# Append train & validation data\n",
    "D_train = D_train + D_valid\n",
    "y_train = y_train + y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBoWFeaturizer...\n",
      "Test accuracy for BBoWFeaturizer  0.8428\n"
     ]
    }
   ],
   "source": [
    "print('BBoWFeaturizer...')\n",
    "featurizer = BBoWFeaturizer()\n",
    "X_train = convert_to_features(D_train, featurizer, vocab)\n",
    "p_y, p_v_y = train_naive_bayes(X_train, y_train, BBoW_k, vocab)\n",
    "preds, conf = predict_naive_bayes(D_test, p_y, p_v_y)\n",
    "acc = accuracy_score(preds, y_test)\n",
    "print('Test accuracy for BBoWFeaturizer ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBoWFeaturizer...\n",
      "Test accuracy for CBoWFeaturizer  0.8488\n"
     ]
    }
   ],
   "source": [
    "print('CBoWFeaturizer...')\n",
    "featurizer = CBoWFeaturizer()\n",
    "X_train = convert_to_features(D_train, featurizer, vocab)\n",
    "p_y, p_v_y = train_naive_bayes(X_train, y_train, CBoW_k, vocab)\n",
    "preds, conf = predict_naive_bayes(D_test, p_y, p_v_y)\n",
    "acc = accuracy_score(preds, y_test)\n",
    "print('Test accuracy for CBoWFeaturizer ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDFFeaturizer...\n",
      "Test accuracy for CBoWFeaturizer  0.812\n"
     ]
    }
   ],
   "source": [
    "print('TFIDFFeaturizer...')\n",
    "idf = compute_idf(D_train, vocab)\n",
    "featurizer = TFIDFFeaturizer(idf)\n",
    "X_train = convert_to_features(D_train, featurizer, vocab)\n",
    "p_y, p_v_y = train_naive_bayes(X_train, y_train, TFIDF_k, vocab)\n",
    "preds, conf = predict_naive_bayes(D_test, p_y, p_v_y)\n",
    "acc = accuracy_score(preds, y_test)\n",
    "print('Test accuracy for CBoWFeaturizer ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
